{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cd36b0-7b73-43d7-b48a-5da7f5cbc944",
   "metadata": {},
   "source": [
    "# Combining CNN & LSTM\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a60f65-ffd2-4f14-a820-fcfdf1259d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating Systems\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Tokenization\n",
    "import nltk\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca22915-d673-4385-9913-2a55657c1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef60c7-eef8-4dfd-b706-aafea7c11402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "--2024-12-21 09:03:36--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.84.20, 54.231.135.41, 16.182.103.217, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.84.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252872794 (241M) [application/zip]\n",
      "Saving to: ‘./data_dir/annotations_trainval2014.zip’\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.16M  21.6MB/s    in 11s     \n",
      "\n",
      "2024-12-21 09:03:48 (22.0 MB/s) - ‘./data_dir/annotations_trainval2014.zip’ saved [252872794/252872794]\n",
      "\n",
      "--2024-12-21 09:03:48--  http://images.cocodataset.org/zips/train2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.30.22, 3.5.25.235, 3.5.27.249, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.30.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13510573713 (13G) [application/zip]\n",
      "Saving to: ‘./data_dir/train2014.zip’\n",
      "\n",
      "train2014.zip        33%[=====>              ]   4.17G  25.7MB/s    eta 10m 14s"
     ]
    }
   ],
   "source": [
    "# linux\n",
    "!apt-get install wget\n",
    "# mac\n",
    "# !brew install wget\n",
    " \n",
    "# create a data directory\n",
    "!mkdir data_dir\n",
    " \n",
    "# download images and annotations to the data directory\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P ./data_dir/\n",
    "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data_dir/\n",
    "!wget http://images.cocodataset.org/zips/val2014.zip -P ./data_dir/\n",
    "    \n",
    "# extract zipped images and annotations and remove the zip files\n",
    "!unzip ./data_dir/annotations_trainval2014.zip -d ./data_dir/\n",
    "!rm ./data_dir/annotations_trainval2014.zip\n",
    "!unzip ./data_dir/train2014.zip -d ./data_dir/\n",
    "!rm ./data_dir/train2014.zip \n",
    "!unzip ./data_dir/val2014.zip -d ./data_dir/ \n",
    "!rm ./data_dir/val2014.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd1641-4ec9-4bd2-9dc7-1bdc18e8e983",
   "metadata": {},
   "source": [
    "### Prediction Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df48258-0516-45c7-a0e4-df9415827180",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = 'sample.jpg'\n",
    " \n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "\n",
    "def load_image(image_file_path, transform=None):\n",
    "    img = Image.open(image_file_path).convert('RGB')\n",
    "    img = img.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        img = transform(img).unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    " \n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "\n",
    "# Build models\n",
    "encoder_model = CNNModel(256).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
    "encoder_model = encoder_model.to(device)\n",
    "decoder_model = decoder_model.to(device)\n",
    "\n",
    "\n",
    "# Load the trained model parameters\n",
    "encoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\n",
    "decoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n",
    "\n",
    "\n",
    "# Prepare an image\n",
    "img = load_image(image_file_path, transform)\n",
    "img_tensor = img.to(device)\n",
    "\n",
    "\n",
    "# Generate an caption from the image\n",
    "feat = encoder_model(img_tensor)\n",
    "sampled_indices = decoder_model.sample(feat)\n",
    "sampled_indices = sampled_indices[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "\n",
    "\n",
    "# Convert word_ids to words\n",
    "predicted_caption = []\n",
    "for token_index in sampled_indices:\n",
    "    word = vocabulary.i2w[token_index]\n",
    "    predicted_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "predicted_sentence = ' '.join(predicted_caption)\n",
    "\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print (predicted_sentence)\n",
    "img = Image.open(image_file_path)\n",
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d32b26-9ef6-47d5-8798-f303ad2121e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
