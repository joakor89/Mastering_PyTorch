{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af44a45c-b1c5-4a4a-89e3-db116f45f608",
   "metadata": {},
   "source": [
    "# Tex Generation\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9121cb-3ab3-4ec0-94bf-47c869501fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "# Data Manipulation\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Network\n",
    "import networkx as nx\n",
    "\n",
    "# Scikit-Learn\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7e91ec-dfa9-462a-9ef9-811e17d73f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda's\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e83f8e1-8c47-48ef-951e-3ea360a555da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8530a197-62fd-4306-ad16-df617d10624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.datasets import PennTreebank\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970fd1e-2006-4418-8221-b5f59fb408ba",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f26b5f-0aec-419c-91cd-a562ce2e3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source, mask_source):\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op\n",
    "\n",
    "def gen_sqr_nxt_mask(size):\n",
    "    msk = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417d340-6c3a-4e99-aed6-65d07cb67ec2",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5f9b0a-7b07-4a4b-9f1b-d4d4b7c4354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, 1, d_m)\n",
    "        pos = torch.arange(size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a1694-a202-4c9a-b31a-0c9e687151d9",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7fcbcf-90d1-4430-8226-fcd808a44b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_iter = PennTreebank(split='train')\n",
    "tkzer = get_tokenizer('basic_english')\n",
    "vocabulary = build_vocab_from_iterator(map(tkzer, tr_iter), specials=['<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_data(raw_text):\n",
    "    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\n",
    "\n",
    "tr_iter, val_iter, te_iter = PennTreebank()\n",
    "training_text = process_data(tr_iter)\n",
    "validation_text = process_data(val_iter)\n",
    "testing_text = process_data(te_iter)\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    text_dataset = text_dataset[:num_batches * batch_size]\n",
    "    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cb8b7e-dc03-40e7-97d0-747116722ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a8af6-b3ca-4f50-86e4-059e12111cf9",
   "metadata": {},
   "source": [
    "### Placing HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b23d87-848c-4ec6-9316-43fc2ae51d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocabulary) \n",
    "embedding_size = 256 \n",
    "num_hidden_params = 256 \n",
    "num_layers = 2 \n",
    "num_heads = 2 \n",
    "dropout = 0.25 \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 10.0 \n",
    "\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9374230-029d-4ff1-9977-4543efd3bacb",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe28d409-4f00-43b8-a1ce-f9f1291f0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    num_batches = len(training_data) // max_seq_len\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        sequence_length = train_data_batch.size(0)\n",
    "        if sequence_length != max_seq_len:  # only on last batch\n",
    "            mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "        op = transformer_model(train_data_batch, mask_source)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        optim_module.zero_grad()\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087af186-5a95-40a4-82d0-17a68bf43451",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fc2d13-c68f-460a-ac04-ff5fca1adcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            sequence_length = eval_data.size(0)\n",
    "            if sequence_length != max_seq_len:\n",
    "                mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "            op = eval_model_obj(eval_data, mask_source)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += sequence_length * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4edf3e3a-2d20-4111-800a-6291e46e72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 50\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e55da6-9a73-4280-bb46-a60fa815fd5f",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15481759-5749-4cfa-b45a-d2998c4b18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af663300-d836-489d-adb7-426206f30295",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_pth = './transformer.pth'\n",
    "\n",
    "torch.save(best_model_so_far.state_dict(), mdl_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289f19a-3db3-4cff-8bad-40a61f1bc0c1",
   "metadata": {},
   "source": [
    "### Loading `The Best Trained Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce92bf06-0b32-43b2-8376-89e4c8fcacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_cached = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "\n",
    "transformer_cached.load_state_dict(torch.load(mdl_pth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df6566f5-c83a-499e-82f3-aab41a14d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = 5\n",
    "sntc = 'They are _'\n",
    "sntc_split = sntc.split()\n",
    "torch.manual_seed(34)\n",
    "mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(ln):\n",
    "        sntc = ' '.join(sntc_split)\n",
    "        txt_ds = Tensor(vocabulary(sntc_split)).unsqueeze(0).to(torch.long)\n",
    "        num_b = txt_ds.size(0)\n",
    "        txt_ds = txt_ds.narrow(0, 0, num_b)\n",
    "        txt_ds = txt_ds.view(1, -1).t().contiguous().to(device)\n",
    "        ev_X, _ = return_batch(txt_ds, i+1)\n",
    "        sequence_length = ev_X.size(0)\n",
    "        if sequence_length != max_seq_len:\n",
    "            mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "        op = transformer_cached(ev_X, mask_source)\n",
    "        op_flat = op.view(-1, num_tokens)\n",
    "        res = vocabulary.get_itos()[op_flat.argmax(1)[0]]\n",
    "        sntc_split.insert(-1, res)\n",
    "print(sntc[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e68b0-8285-48ba-b27a-e949a1b93eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190bc22-e136-4bd8-972a-023dc3e95893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
