{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57785e86-1192-470b-9e07-fc069e92afe7",
   "metadata": {},
   "source": [
    "# Overview of Deep Learning using PyTorch\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf6c2d6-0bd1-49f7-a1a9-345767515df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7e8a09-7f32-425f-8828-26785301142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "Torchvision version: 0.20.1\n",
      "Torchaudio version: 2.5.1\n",
      "GPU Existance: False\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"Torchaudio version:\", torchaudio.__version__)\n",
    "\n",
    "print(\"GPU Existance:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879c65f-8a5e-42f6-87f1-7454a466ede0",
   "metadata": {},
   "source": [
    "### Simple Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3935d170-df92-45ca-9648-436292ab0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f587c3-f9ec-4c57-81f9-f1eb1ea39284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f7d9ad-7ddd-49a8-a840-d5aff95993f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd485223-acec-4bf0-8bb4-e3393fca7729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([1., 4., 2., 1., 3., 5.])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9136a28-f9a7-48ec-9394-f757701ec7b0",
   "metadata": {},
   "source": [
    "### PyTorch Modules\n",
    "\n",
    "#### `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2856b2a8-ac17-4e1c-bc00-dc2767226b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(256, 4) / math.sqrt(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f065a01a-7871-48f7-8de0-93d7029baf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1346, -0.0698,  0.1033,  0.0662],\n",
       "        [ 0.1340,  0.0318,  0.0676, -0.0182],\n",
       "        [ 0.0436,  0.0436, -0.0068, -0.0644],\n",
       "        ...,\n",
       "        [ 0.1246, -0.0227,  0.1562, -0.0856],\n",
       "        [-0.0402, -0.0930,  0.1501, -0.0128],\n",
       "        [-0.0266, -0.0851,  0.0863,  0.0487]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4532f3d1-5dff-4877-908d-ac83d34884ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = torch.zeros(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf60fb7-0f38-434d-9010-b0fd2e71107a",
   "metadata": {},
   "source": [
    "### Training a Neural Network using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd35539-9431-47d4-a0b3-65a4a30e1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e7122d-c03d-461e-8c15-f5f8c497924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Dropout2d(0.10),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(4608, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41496ec8-de01-4d5a-94d7-9be1e88b06f6",
   "metadata": {},
   "source": [
    "### Training Model Function\n",
    "\n",
    "##### `nll` is the negative likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d11f7b52-d382-492a-bb63-d7ac91e74c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960c1a1-a36a-4bb7-adf6-ee9837dba13b",
   "metadata": {},
   "source": [
    "#### `Test Model Performance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669abe59-31d1-403d-95e8-c8323d021ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            # loss summed across the batch\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  \n",
    "            # Use ArgMax to get the most likely prediction\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  \n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6c354-79af-42e4-8e46-3ec040fe3d69",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "1. The `Mean` & `Standard Deviation` values are calculated as the mean of all pixel values of all images in the Training Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a26f743-8cd3-4768-8f0b-9f37dbc1c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|██████████████████████████████████████| 9.91M/9.91M [00:05<00:00, 1.77MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|███████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 384kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|██████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 2.70MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 1.09MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eebfe1-777c-49d4-b663-141f28385006",
   "metadata": {},
   "source": [
    "### Placing Optimizer & Running Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9482a235-5eb0-4026-9d3d-619d85c13a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ce1de-179f-4f27-b0a7-a17bd487130d",
   "metadata": {},
   "source": [
    "### Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ea40ea3-0814-45a2-ab5f-1cfad70b895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isisromero/anaconda3/envs/Pytorch/lib/python3.11/site-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301478\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.744152\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.234419\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.831864\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.969942\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.553293\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.627297\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.533486\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.450610\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.363707\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.340398\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.372920\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.330355\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.446091\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.415660\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.467097\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.183409\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.303824\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.131208\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.388443\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.302601\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.082613\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.275926\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.074649\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.249466\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.190864\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.362660\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.104137\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.074714\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.261259\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.285470\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.256550\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.191824\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.483997\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.099164\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.161573\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.289485\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.043149\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.147299\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.048394\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.125211\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.107071\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.075411\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.049025\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.030186\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.123858\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.178767\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.114949\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.185496\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.238971\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.149143\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.061175\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.040555\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.092657\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.049758\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.089874\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.042555\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.039767\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.080775\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.318996\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.112008\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.126404\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.102587\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.066925\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.100134\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.073945\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.354822\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.078190\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.157125\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.068998\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.211374\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.269127\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.021048\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.212556\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.322844\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.099037\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.036574\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.207974\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.054981\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.025745\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.339903\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.492840\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.022692\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.060657\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.166840\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.046820\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.012527\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.133060\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.003951\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.152644\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.086572\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.512170\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.010440\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.084594\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.365422\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.047370\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.067610\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.073676\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.049595\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.026395\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.003475\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.053469\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.229894\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.004622\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.412983\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.100508\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.040076\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.062282\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.008279\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.140918\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.173995\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.109411\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.033980\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.006718\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.061822\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.173779\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.167240\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.134696\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.015234\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.114887\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.113140\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.252375\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.020415\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.268211\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.019326\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.102987\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.047051\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.183867\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.165664\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.277949\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.168438\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.021313\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.010087\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.024996\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.131787\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.044595\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.114822\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.100224\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.168980\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.080848\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.284294\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.096321\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.143684\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.014815\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.494407\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.069971\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.181275\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.128308\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.112727\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.122312\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.167759\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.173495\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.019931\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.007523\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.189103\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.062305\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.011331\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.136982\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.004384\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.013983\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.239069\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.005193\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.024250\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.014795\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.015885\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.055773\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.009126\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.084744\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.098472\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.010830\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.110808\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.060927\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.002731\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.114298\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.014549\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.083702\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.053858\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.063041\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.028975\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.064457\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.049814\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.059694\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.046733\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.020659\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.054080\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.004909\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.038102\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.028216\n",
      "\n",
      "Test dataset: Overall Loss: 0.0479, Overall Accuracy: 9842/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.108304\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.037673\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.155213\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.148759\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.043140\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.017571\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.048599\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.003854\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.012026\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.061445\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.013110\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.156311\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.059251\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.212637\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.000450\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.045471\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.217845\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.154735\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.010518\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.159202\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.053469\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.077108\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.148390\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.039692\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.122008\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.048593\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.029562\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.056022\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.474408\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.104461\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.023668\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.012077\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.005518\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.071233\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.019190\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.031548\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.036915\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.022836\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.017355\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.050561\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.050156\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.154811\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.027702\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.014755\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.040699\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.028510\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.030638\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.052812\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.180396\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.011156\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.016128\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.018557\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.048719\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.008175\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.053388\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.132398\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.000573\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.064857\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.006516\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.020726\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.011506\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.013922\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.153639\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.006770\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.007559\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.023542\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.065134\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.022995\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.046601\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.085207\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.001838\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.002970\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.022641\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.004951\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.012083\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.054331\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.002353\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.050491\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.073544\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.005048\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.013300\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.001454\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.106127\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.034581\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.050462\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.009745\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.028535\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.033330\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.117751\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.061651\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.007306\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.039946\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.142707\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.036589\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.003902\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.035635\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.355220\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.005321\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.021628\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.052086\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.197678\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.045932\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.108175\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.042176\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.007241\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.371629\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.038575\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.047499\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.049816\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.018446\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.012702\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.004561\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.006350\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.098999\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.073167\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.029670\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.352322\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.078285\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.118320\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.066743\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.114414\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.013779\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.009895\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.042164\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.001508\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.107723\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.028706\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.021824\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.002223\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.044938\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.028136\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.001732\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.000281\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.012164\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.017530\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.030778\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.092812\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.010540\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.145814\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.013461\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.022409\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.001566\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.132943\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.061105\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.185826\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.263311\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.056822\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.003017\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.009440\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.128829\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.245224\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.022969\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.008636\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.073290\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.047963\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.063398\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.019094\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.081679\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.004610\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.309505\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.004231\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.012585\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.495107\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.038626\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.052876\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.339818\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.017714\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.170498\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.056349\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.018604\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.079427\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.381515\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.009129\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.022358\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.178029\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.027444\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.202448\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.434431\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.003990\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.017385\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.077425\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.010727\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.075864\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.009914\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.005252\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.171088\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.109657\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.003177\n",
      "\n",
      "Test dataset: Overall Loss: 0.0415, Overall Accuracy: 9857/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866a3f1-a274-4d24-b4a8-bdc5c189701c",
   "metadata": {},
   "source": [
    "### Executing `Inference` on `Trained Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b54d227-784d-423e-adcb-6060a9304250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46015bc9-8259-4735-a08d-46faecf791a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68527da1-fac6-464b-8427-78a8715db6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
