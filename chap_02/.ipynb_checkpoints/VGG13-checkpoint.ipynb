{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbaf433-70a2-4546-a76f-d1b4a3a28e70",
   "metadata": {},
   "source": [
    "# VGG13 Architecture\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68a14e8-1503-4a85-890d-8fac5e4b2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating Systems\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c89b289-6157-4acb-8d89-e967cac45003",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5867f8-78f7-492f-a546-0cc4a61b5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = 'hymenoptera_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113306c6-ec26-4b33-a222-9dbd1848194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# with open('./imagenet1000_clsidx_to_labels.txt') as f:\n",
    "#     classes_data = f.read()\n",
    "# classes_dict = ast.literal_eval(classes_data)\n",
    "# print({k: classes_dict[k] for k in list(classes_dict)[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dbf8f-a484-436f-a1b6-ebfbdb260c8a",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf97cae-6c0e-4e34-b17c-dbb3378b0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformers = {\n",
    "#     'train': transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(),\n",
    "#                                     transforms.ToTensor(), \n",
    "#                                     transforms.Normalize([0.490, 0.449, 0.411], [0.231, 0.221, 0.230])]),\n",
    "#     'val': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), \n",
    "#                                       transforms.ToTensor(), \n",
    "#                                       transforms.Normalize([0.490, 0.449, 0.411], [0.231, 0.221, 0.230])])}\n",
    "\n",
    "# img_data = {k: datasets.ImageFolder(os.path.join(ddir, k), data_transformers[k]) for k in ['train', 'val']}\n",
    "# dloaders = {k: torch.utils.data.DataLoader(img_data[k], batch_size=8, shuffle=True, num_workers=2) \n",
    "#             for k in ['train', 'val']}\n",
    "# dset_sizes = {x: len(img_data[x]) for x in ['train', 'val']}\n",
    "# dvc = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2b875f-2ee8-4e15-a02b-61fe0d3b6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageshow(img, text=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    avg = np.array([0.490, 0.449, 0.411])\n",
    "    stddev = np.array([0.231, 0.221, 0.230])\n",
    "    img = stddev * img + avg\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if text is not None:\n",
    "        plt.title(text)\n",
    "\n",
    "def visualize_predictions(pretrained_model, max_num_imgs=4):\n",
    "    torch.manual_seed(1)\n",
    "    was_model_training = pretrained_model.training\n",
    "    pretrained_model.eval()\n",
    "    imgs_counter = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, tgts) in enumerate(dloaders['val']):\n",
    "            imgs = imgs.to(dvc)\n",
    "            ops = pretrained_model(imgs)\n",
    "            _, preds = torch.max(ops, 1)\n",
    "            \n",
    "            for j in range(imgs.size()[0]):\n",
    "                imgs_counter += 1\n",
    "                ax = plt.subplot(max_num_imgs//2, 2, imgs_counter)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'pred: {classes_dict[int(preds[j])]}')\n",
    "                imageshow(imgs.cpu().data[j])\n",
    "\n",
    "                if imgs_counter == max_num_imgs:\n",
    "                    pretrained_model.train(mode=was_model_training)\n",
    "                    return\n",
    "        pretrained_model.train(mode=was_model_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d8e39b-843f-440f-b073-efa64290e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg13(pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2eab92-795d-47b8-b976-3d726a07e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf9a53-3541-4609-bac4-8efcb93e425e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
