{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e096306d-f190-48a3-b230-8ea8f90c62e3",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7d914e-95e0-4bae-b7be-57d7fe16cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating Systems\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# Network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c31fff9-85ad-4868-aa68-894c49448616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda's\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777d485c-1608-4e0d-b050-6a468552c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a708a4cb-5f25-400d-af6a-6cd89ed50fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PyTorch Text\n",
    "# from torchtext.datasets import PennTreebank\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd49bb4-085b-4659-838e-63a841ebabc4",
   "metadata": {},
   "source": [
    "### `Transformer` Class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565801a5-0556-4f30-b044-2d80cce44ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source, mask_source):\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op\n",
    "\n",
    "def gen_sqr_nxt_mask(size):\n",
    "    msk = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c3c38-bfb8-4c56-b285-7fa6001528f0",
   "metadata": {},
   "source": [
    "### Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb7ab1c-f225-4e40-b4a9-8569164aed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, 1, d_m)\n",
    "        pos = torch.arange(size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "783bebbc-b4e0-4af1-b931-e2a23d23a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_iter = PennTreebank(split='train')\n",
    "tkzer = get_tokenizer('basic_english')\n",
    "vocabulary = build_vocab_from_iterator(map(tkzer, tr_iter), specials=['<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_data(raw_text):\n",
    "    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\n",
    "\n",
    "tr_iter, val_iter, te_iter = PennTreebank()\n",
    "training_text = process_data(tr_iter)\n",
    "validation_text = process_data(val_iter)\n",
    "testing_text = process_data(te_iter)\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    text_dataset = text_dataset[:num_batches * batch_size]\n",
    "    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a71f08b6-a3a0-455a-bfe9-66b18e159b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d513fc6d-5114-4532-a471-61ecfd5c14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocabulary) \n",
    "embedding_size = 256 \n",
    "num_hidden_params = 256 \n",
    "num_layers = 2 \n",
    "num_heads = 2 \n",
    "dropout = 0.25 \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 \n",
    "\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68620ec1-e854-40b9-ad43-84fef3746bfe",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9565fe-7d43-4c93-9736-c27ea77ce6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
